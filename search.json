[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Máquina para Riesgos",
    "section": "",
    "text": "Temario y referencias\nTodas las notas y material del curso estarán en este repositorio.\n\nIntroducción al aprendizaje máquina\nSesgo y varianza. Métodos locales y estructurados. Ingeniería de variables de entrada.\nMétodos de remuestreo y validación cruzada\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nÁrboles, bosques aleatorios y boosting\nRedes neuronales\nDiagnóstico y mejora en problemas de aprendizaje supervisado\nAnálisis de conglomerados\n\n\nEvaluación\n\nTareas semanales (40%) para discutir en clase, compartidas en el repositorio y en nuestro espacio de trabajo de Posit Cloud.\nUn examen final (40% práctico, 20% teórico), con una componente oral.\n\n\n\nMaterial\nEl material de este trimestre está en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2014)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\nOtras referencias\n\nDeep Learning, Goodfellow, Bengio, y Courville (2016)\nTidy Modeling with R, Kuhn y Silge (2022)\nPredicción conforme\n\n\n\nSoftware\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje o flujo de trabajo que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. En lo posible utilizamos librerías especializadas que se pueden utilizar desde varias plataformas (keras, por ejemplo).\n\n\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., y J. Silge. 2022. Tidy Modeling with R. O’Reilly Media. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ.",
    "crumbs": [
      "Temario y referencias"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 ¿Qué es aprendizaje de máquina (machine learning)?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. Nos interesan principalmente aquellas tareas o decisiones que se toman de manera repetida y donde el costo de cada error es relativamente bajo.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\nLas tareas más apropiadas para este enfoque, en general, son aquellas en donde:\nAunque es posible extender lo que veremos a contextos más generales, cuando uno de los tres anteriores requisitos no se cumple es mejor usar o complementar con otros enfoques.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "title": "1  Introducción",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera más o menos homogénea una cantidad grande de veces.\n\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses.\nReconocer palabras escritas a mano (OCR).\nDetectar llamados de ballenas en grabaciones de boyas.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "href": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "title": "1  Introducción",
    "section": "1.2 Ejemplo: reglas y aprendizaje",
    "text": "1.2 Ejemplo: reglas y aprendizaje\nLectura de un medidor mediante imágenes. Supongamos que en una infraestructura donde hay medidores análogos (de agua, electricidad, gas, etc.) que no se comunican. ¿Podríamos pensar en utilizar fotos tomadas automáticamente para medir el consumo?\nPor ejemplo, consideramos el siguiente problema (tomado de aquí, ver código y datos):\n\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(437)\npath_img &lt;- \"../datos/medidor/\"\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\nmedidor &lt;- load.image(sample(path_full_imgs, 1))\npar(mar = c(1, 1, 1, 1))\nplot(medidor, axes = FALSE)\n\n\n\n\n\n\n\n\nNótese que las imágenes y videos son matrices o arreglos de valores de pixeles, por ejemplo estas son las dimensiones para una imagen:\n\ndim(medidor)\n\n[1] 142 142   1   3\n\n\nEn este caso, la imagen es de 193 x 193 pixeles y tiene tres canales, o tres matrices de 193 x 193 donde la entrada de cada matriz es la intensidad del canal correspondiente. Buscámos hacer cálculos con estas matrices para extraer la información que queremos. En este caso, construiremos estos cálculos a mano.\nPrimero filtramos (extraemos canal rojo y azul, restamos, difuminamos y aplicamos un umbral):\n\nmedidor_rojo &lt;- medidor |&gt;  R() \nmedidor_azul &lt;- medidor |&gt; B()\nmedidor_1 &lt;- (medidor_rojo - medidor_azul) |&gt; isoblur(5)\naguja &lt;-  medidor_1 |&gt;  imager::threshold(\"90%\", approx = FALSE)\n\n\n\n\n\n\n\n\n\n\nLogramos extraer la aguja, aunque hay algo de ruido adicional. Una estrategia es extraer la componente conexa más grande (que debería corresponder a la aguja), y luego calcular su orientación. Una manera fácil es encontrar una recta que vaya del centro de la imagen hasta el punto más alejado del centro (aunque quizá puedes pensar maneras más robustas de hacer esto):\n\ncalcular_punta &lt;- function(pixset){\n  centro &lt;- floor(dim(pixset)[1:2] / 2)\n  # segmentar en componentes conexas\n  componentes &lt;- split_connected(pixset)\n  # calcular la más grande\n  num_pixeles &lt;- map_dbl(componentes, sum)\n  ind_maxima &lt;- which.max(num_pixeles)\n  pixset_tbl &lt;- as_tibble(componentes[[ind_maxima]]) |&gt; \n    mutate(dist = (x - centro[1])^2 + (y - centro[2])^2) |&gt; \n    top_n(1, dist)  |&gt; \n    mutate(x_1 = x - centro[1], y_1 = y - centro[2])\n  pixset_tbl[1, ] \n}\n\nY ahora podemos aplicar el proceso de arriba a todas la imágenes:\n\npath_imgs &lt;- list.files(path = path_img)\n\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\n# en este caso los datos están etiquetados\ny_imagenes &lt;- path_imgs |&gt; str_sub(1, 3) |&gt; as.numeric()\n# procesar algunas imagenes\nset.seed(82)\nindice_imgs &lt;- sample(1:length(path_full_imgs), 500)\nangulos &lt;- path_full_imgs[indice_imgs] |&gt; \n    map( ~ load.image(.x)) |&gt;  \n    map(~ R(.x) - B(.x)) |&gt; \n    map( ~ isoblur(.x, 5)) |&gt; \n    map( ~ imager::threshold(.x, \"90%\")) |&gt; \n    map( ~ calcular_punta(.x)) |&gt; \n  bind_rows()\n\n\nangulos_tbl &lt;- angulos |&gt; \n  mutate(y_medidor = y_imagenes[indice_imgs])\nggplot(angulos_tbl, \n    aes(x = 180 * atan2(y_1, x_1) / pi + 90, y = y_medidor)) +\n  geom_point() + xlab(\"Ángulo\")\n\n\n\n\n\n\n\n\nEl desempeño no es muy malo pero tiene algunas fallas grandes. Quizá refinando nuestro pipeline de procesamiento podemos mejorarlo.\n\nPor el contrario, en el enfoque de aprendizaje, comenzamos con un conjunto de datos etiquetado (por una persona, por un método costoso, etc.), y utilizamos alguna estructura general para aprender a producir la respuesta a partir de las imágenes. Por ejemplo, en este caso podríamos una red convolucional sobre los valores de los pixeles de la imagen:\n\nlibrary(keras3)\n# usamos los tres canales de la imagen\nimagenes &lt;- map(path_full_imgs, ~ image_load(.x, target_size = c(64L, 64L)))\nimgs_array &lt;-  imagenes |&gt; map(~ image_to_array(.x)) \nimgs_array &lt;- map(imgs_array, ~ array_reshape(.x, c(1, 64, 64, 3)))\nx &lt;- abind::abind(imgs_array, along = 1)\nset.seed(25311)\nindices_entrena &lt;- sample(1:dim(x)[1], size = 4200)\nx_entrena &lt;- x[indices_entrena,,,]\ny_entrena &lt;- y_imagenes[indices_entrena] / 10\nx_valida &lt;- x[-indices_entrena,,,]\ny_valida &lt;- y_imagenes[-indices_entrena] / 10\n\n\nmodelo_aguja &lt;- keras_model_sequential() |&gt;\n  layer_rescaling(1./255) |&gt;\n  layer_random_rotation(0.05, fill_mode = \"nearest\") |&gt;\n  layer_random_zoom(0.05, fill_mode = \"nearest\") |&gt;\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt;\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_conv_2d(filters = 16, kernel_size = c(3, 3)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_flatten() |&gt; \n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 1, activation = 'linear')\n\nAjustamos el modelo:\n\nmodelo_aguja |&gt; compile(\n  loss = \"mse\",\n  optimizer = optimizer_adam(learning_rate = 0.0005),\n  metrics = list(metric_mean_absolute_error())\n)                                                                                                        \n# Entrenar\nmodelo_aguja |&gt; fit(\n  x_entrena, y_entrena,\n  epochs = 200,\n  verbose = TRUE, \n  batch_size = 64,\n  validation_data = list(x_valida, y_valida)\n)\n#save_model(modelo_aguja, \"cache/modelo-aguja.keras\")\n\n\nmodelo &lt;- load_model(\"cache/modelo-aguja.keras\")\nmodelo\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ rescaling (Rescaling)             │ (None, 64, 64, 3)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ random_rotation (RandomRotation)  │ (None, 64, 64, 3)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ random_zoom (RandomZoom)          │ (None, 64, 64, 3)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)                 │ (None, 60, 60, 32)       │         2,432 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_2 (MaxPooling2D)    │ (None, 30, 30, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_3 (Conv2D)                 │ (None, 26, 26, 32)       │        25,632 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_3 (MaxPooling2D)    │ (None, 13, 13, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_4 (Conv2D)                 │ (None, 11, 11, 16)       │         4,624 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_4 (MaxPooling2D)    │ (None, 5, 5, 16)         │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten_1 (Flatten)               │ (None, 400)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)               │ (None, 400)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 100)              │        40,100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)               │ (None, 100)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ (None, 100)              │        10,100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)               │ (None, 100)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 1)                │           101 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 248,969 (972.54 KB)\n Trainable params: 82,989 (324.18 KB)\n Non-trainable params: 0 (0.00 B)\n Optimizer params: 165,980 (648.36 KB)\n\n\nY observamos que obtenemos predicciones prometedoras:\n\npreds &lt;- predict(modelo, x[-indices_entrena,,,])\n\n23/23 - 1s - 24ms/step\n\npreds_tbl &lt;- tibble(y = y_imagenes[-c(indices_entrena)] / 10, preds = preds)\nggplot(preds_tbl, aes(x = preds, y = y)) +\n  geom_jitter(alpha = 0.5) +\n  geom_abline(colour = 'red')\n\n\n\n\n\n\n\n\nDe forma que podemos resolver este problema con algoritmos generales, sin tener que aplicar métodos sofisticados de procesamiento de imágenes. El enfoque de aprendizaje es particularmente efectivo cuando hay cantidades grandes de datos poco ruidosos, y aunque en este ejemplo los dos enfoques dan resultados razonables, en procesamiento de imágenes es cada vez más común usar redes neuronales grandes para resolver este tipo de problemas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#sec-medicioncostosa",
    "href": "01-introduccion.html#sec-medicioncostosa",
    "title": "1  Introducción",
    "section": "1.3 Ejemplo: mediciones costosas",
    "text": "1.3 Ejemplo: mediciones costosas\nEn algunos casos, el estándar de la medición que nos interesa es uno que es costoso de cumplir: a veces se dice que etiquetar los datos es costoso. Un ejemplo es producir las estimaciones de ingreso trimestral de un hogar que se recolecta en la ENIGH (ver aquí). En este caso particular, se utiliza esta encuesta como datos etiquetados para poder estimar el ingreso de otros hogares que no están en la muestra del ENIGH, pero para los que se conocen características de las vivienda, características de los integrantes, y otras medidas que son más fácilmente recolectadas en encuestas de opinión.\nVeremos otro ejemplo: estimar el valor de mercado de las casas en venta de una región. Es posible que tengamos un inventario de casas con varias de sus características registradas, pero producir estimaciones correctas de su valor de mercado puede requerir de inspecciones costosas de expertos, o tomar aproximaciones imprecisas de esta cantidad (por ejemplo, cuál es el precio ofertado).\nUtilizaremos datos de casas que se vendieron en Ames, Iowa en cierto periodo. En este caso, conocemos el valor a la que se vendió una casa. Buscamos producir una estimación para otras casas para las cuales conocemos características como su localización, superficie en metros cuadrados, año de construcción, espacio de estacionamiento, y así sucesivamente. Estas medidas son más fáciles de recolectar, y quisiéramos producir una estimación de su precio de venta en términos de estas medidas.\nEn este ejemplo intentaremos una forma simple de predecir.\n\nlibrary(tidymodels)\nlibrary(patchwork)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split &lt;- initial_split(casas, prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena &lt;- training(casas_split)\n# graficar\ng_1 &lt;- ggplot(casas_entrena, aes(x = precio_miles)) +\n  geom_histogram()\ng_2 &lt;- ggplot(casas_entrena, aes(x = area_hab_m2, \n                          y = precio_miles, \n                          colour = condicion_venta)) +\n  geom_point() \ng_1 + g_2\n\n\n\n\n\n\n\n\nLa variable de condición de venta no podemos utilizarla para predecir, pues sólo la conocemos una vez que la venta se hace. Sin embargo, nos interesa principalemente entender qué sucede cuando las casas se venden en condiciones normales. Consideramos además del área habitable, por ejemplo, la calidad general de terminados:\n\nggplot(casas_entrena |&gt;  \n       filter(condicion_venta == \"Normal\") |&gt;  \n       mutate(calidad_grupo = \n        cut(calidad_gral, breaks = c(0, 5, 7, 8, 10))), \n  aes(x = area_hab_m2, \n      y = precio_miles,\n      colour = calidad_grupo)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\nPrecio vs área y calidad\n\n\n\n\nVemos que estas dos variables que hemos usado explican buena parte de la variación de los precios de las casas. Podemos examinar otras variables como la existencia y tamaño del garage:\n\nggplot(casas_entrena |&gt;  filter(condicion_venta == \"Normal\"),\n       aes(x = area_hab_m2, y = precio_miles, colour = area_garage_m2)) +\n  geom_point(alpha = 0.5) + facet_wrap(~ (area_garage_m2 &gt; 0))\n\n\n\n\n\n\n\n\nY quizá podríamos proponer una fórmula simple de la forma:\n\\[Precio = a[\\textrm{Calidad}] + b[\\textrm{Calidad}]\\, Area + c\\, \\textrm{AreaGarage} + d\\,\\textrm{TieneGarage}\\]\ndonde los valores de \\(a[\\textrm{Calidad}] , b[\\textrm{Calidad}], c, d\\) podríamos estimarlos de los datos. La pendiente de Area dependende de la calificación de la calidad de los terminados.\nNuestro proceso comenzaría entonces construir los datos para usar en el modelo:\n\nreceta_casas &lt;- \n  recipe(precio_miles ~ area_hab_m2 + calidad_gral + \n           area_garage_m2, \n         data = casas_entrena) |&gt;  \n  step_cut(calidad_gral, breaks = c(3, 5, 6, 7, 8)) |&gt;  \n  step_normalize(starts_with(\"area\")) |&gt; \n  step_mutate(tiene_garage = ifelse(area_garage_m2 &gt; 0, 1, 0)) |&gt; \n  step_dummy(calidad_gral) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) \n\nDefinimos el tipo de modelo que queremos ajustar, creamos un flujo y ajustamos\n\n# modelo\ncasas_modelo &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n# flujo\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_modelo)\n# ajustar flujo\najuste &lt;- fit(flujo_casas, casas_entrena)\najuste\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_cut()\n• step_normalize()\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                       (Intercept)                         area_hab_m2  \n                           111.124                              22.505  \n                    area_garage_m2                        tiene_garage  \n                            12.014                               3.230  \n               calidad_gral_X.3.5.                 calidad_gral_X.5.6.  \n                            30.104                              54.623  \n               calidad_gral_X.6.7.                 calidad_gral_X.7.8.  \n                            79.565                             119.639  \n              calidad_gral_X.8.10.   area_hab_m2_x_calidad_gral_X.3.5.  \n                           217.099                              -7.942  \n area_hab_m2_x_calidad_gral_X.5.6.   area_hab_m2_x_calidad_gral_X.6.7.  \n                             2.839                              14.141  \n area_hab_m2_x_calidad_gral_X.7.8.  area_hab_m2_x_calidad_gral_X.8.10.  \n                            14.221                              -1.421  \n\n\nY ahora podemos hacer predicciones para nuevos datos no observados en el entrenamiento:\n\nset.seed(8)\ncasas_prueba &lt;- testing(casas_split) \nejemplos &lt;- casas_prueba|&gt; sample_n(5)\npredict(ajuste, ejemplos) |&gt; \n  bind_cols(ejemplos |&gt; select(precio_miles, area_hab_m2)) |&gt; \n  arrange(desc(precio_miles)) |&gt; gt() |&gt; \n  fmt_number(columns = everything(), decimals = 1)\n\n\n\n\n\n\n\n.pred\nprecio_miles\narea_hab_m2\n\n\n\n\n242.3\n275.0\n152.9\n\n\n177.3\n181.0\n155.6\n\n\n169.3\n175.5\n132.1\n\n\n123.1\n133.0\n117.8\n\n\n115.6\n128.5\n90.2\n\n\n\n\n\n\n\nY finalmente podemos evaluar nuestro modelo. En este casos mostramos diversas métricas como ejemplo:\n\nmetricas &lt;- metric_set(mape, mae, rmse)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste, casas_prueba)), \n     truth = precio_miles, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n14.1\n\n\nmae\nstandard\n23.4\n\n\nrmse\nstandard\n33.3\n\n\n\n\n\n\n\n\ncasas_prueba_f &lt;- filter(casas_prueba,\n  condicion_venta %in% c(\"Normal\", \"Partial\", \"Abnorml\"))\nggplot(casas_prueba_f |&gt;\n       bind_cols(predict(ajuste, casas_prueba_f)),\n       aes(x = .pred, y = precio_miles)) +\n  geom_point() +\n  geom_abline(colour = \"red\") + facet_wrap(~ condicion_venta) +\n  xlab(\"Predicción (miles)\") + ylab(\"Precio (miles)\")\n\n\n\n\n\n\n\n\nEste modelo tiene algunos defectos y todavía tiene error considerablemente grande. La mejora sin embargo podemos cuantificarla con un modelo base o benchmark. En este caso utilizamos el siguiente modelo simple, cuya predicción es el promedio de entrenamiento:\n\n# nearest neighbors es grande, así que la predicción\n# es el promedio de precio en entrenamiento\ncasas_promedio &lt;- nearest_neighbor(\n    neighbors = 1000, weight_func = \"rectangular\") |&gt;\n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\nworkflow_base &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_promedio)\najuste_base &lt;- fit(workflow_base, casas_entrena)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste_base, casas_prueba)), \n     truth = precio_miles, estimate = .pred)|&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n33.4\n\n\nmae\nstandard\n54.8\n\n\nrmse\nstandard\n77.2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.4 Aprendizaje supervisado y no supervisado",
    "text": "1.4 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html",
    "href": "02-principios-supervisado.html",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "2.1 Población y pérdida\nEn esta sección examinaremos algunos principios teóricos y de metodología para el aprendizaje supervisado.\nSupongamos que tenemos una población grande de observaciones potenciales de la forma\n\\[(x_1, x_2, \\ldots, x_p, y) \\]\nY para esa población nos interesa predecir una variable respuesta \\(y\\) numérica en términos de variables de entrada disponibles \\(x = (x_1,x_2,\\ldots, x_p)\\):\n\\[(x_1, x_2, \\ldots, x_p) \\to y\\]\nEl proceso que produce la salida \\(y\\) a partir de las entradas es típicamente muy complejo y dificíl de describir de forma mecanística (por ejemplo, el ingreso dadas características de los hogares, sus integrantes, condiciones económicas, etc).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#población-y-pérdida",
    "href": "02-principios-supervisado.html#población-y-pérdida",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "Ejemplo\nPara ilustrar esta discusión teórica, consideraremos datos simulados. La población está dada por el siguiente proceso generador de datos:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(gt)\ngenera_datos &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x = estudio_años, y = ingreso)\n}\n\nTenemos una sola entrada y una respuesta numérica, y una muestra se ve como sigue:\n\nset.seed(1234)\ndatos_tbl &lt;- genera_datos(n = 500, tipo = \"entrena\")\nggplot(datos_tbl, aes(x = x, y = y)) + geom_jitter(width = 0.3) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")\n\n\n\n\n\n\n\n\n\nBuscamos construir una función \\(f\\) tal que si observamos cualquier \\(x = (x_1, x_2, \\ldots, x_p) \\to y\\), entonces nuestra predicción es\n\\[\\hat{y} = f(x_1, x_2, \\ldots, x_p) = f(x).\\] Con esta regla o algoritmo \\(f\\) queremos predecir con buena precisión el valor de \\(y\\). Esta \\(f\\), como explicamos antes, puede ser producida de muy distintas maneras (experiencia, reglas a mano, datos, etc.)\nNuestra primera tarea es definir qué quiere decir predecir con buena precisión.\nPara hacer esto tenemos que introducir una medida del error, que llamamos en general función de pérdida.\n\n\n\n\n\n\nFunción de pérdida y error de predicción\n\n\n\nSi el verdadero valor observado es \\(y\\) y nuestra predicción es \\(f(x)\\), denotamos la pérdida asociada a esta observación como \\[L(y, f(x))\\] Para medir el desempeño general de la regla \\(f\\), consideramos su valor esperado, el error de predicción, que es el promedio sobre toda la población:\n\\[Err(f) = E[L(y, f(x))]\\]\nEste es el error que obtendríamos promediando las pérdidas sobre toda la población de interés.\n\n\nObservación: Para fijar ideas, podríamos usar por ejemplo la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) o la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\).\nObservación: en aprendizaje automático, evaluación del error casi siempre es un error promedio. Esto es aceptable cuando no hay riesgos de errores graves, y cuando haremos un número considerable de predicciones en el futuro.\nAl menos en teoría, podemos encontrar una \\(f\\) que minimiza esta pérdida:\n\n\n\n\n\n\nPredictor óptimo\n\n\n\nPara una población dada, el predictor óptimo (teórico) es\n\\[f^* = \\underset{f}{\\mathrm{argmin}} E[L(y, f(x))].\\]\nEs decir: el mínimo error posible que podemos obtener es \\(Err(f^*)\\). Para cualquier otro predictor \\(f\\) tenemos que \\(Err(f) \\geq Err(f^*).\\)\n\n\nPor ejemplo si usamos la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\), entonces puede mostrarse que\n\\[f^*(x) = E(y | x)\\] de forma que \\(f^*\\) es la media condicional de la \\(y\\) dado que sabemos que las entradas son \\(x\\). Si usáramos la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) entonces \\[f^*(x) = \\textrm{mediana}(y|x).\\] Distintas funciones de pérdida dan distintas soluciones teóricas. Por ejemplo, si existen valores atípicos en \\(y\\) producidos por errores de registro o medición, usar la pérdida absoluta puede dar mejores resultados que la cuadrática, que tiende a dar mayor peso a errores grandes.\nObservaciones:\n\nPodemos ver nuestra tarea entonces como una de ajuste de curvas: queremos aproximar tan bien como sea posible la función \\(f^*(x)\\).\nNo es simple decidir qué función de pérdida debería utilizarse para un problema dado de predicción.\nGeneralmente es una combinación de costos/beneficios del problema que tratamos, conveniencia computacional, y cómo se comportan los errores de nuestros predictores bajo distintas pérdidas. Sin embargo, al principio del proceso de construcción de modelos es mejor escoger una métrica simple que capture a grandes rasgos el comportamiento que esperamos (pérdida cuadrática, absoluta o logarítmica por ejemplo).\nMuchas veces es mejor considerar el problema de selección de la pérdida desde dos ángulos: el primero es computacional y de propiedades de la predicción, y el segundo tiene que ver con costos y beneficios asociados al problema que queremos resolver. Para el primero, alguna de las pérdidas estándar (como las que vimos arriba, cuadrática y absoluta, o logarítmica) es usualmente suficiente.\nEn el segundo ángulo (el de costos y beneficios, o desde el punto de vista de negocio o área de conocimiento), el análisis involucra más aspectos particulares del problema y generalmente tiene que hacerse de manera ad-hoc.\n\n\n\nEjemplo\nSupongamos que nos interesa minimizar la pérdida cuadrática. Si tomamos una muestra muy grande (para este problema), podemos aproximar la predicción óptima directamente. Abajo graficamos nuestra muestra chica de datos junto con una buena aproximación del predictor óptimo. En este caso, hacemos simplemente el promedio de las \\(y\\) para cada valor de \\(x\\), que funciona razonablemente bien por la muestra grande y la dimensión baja del problema:\n\npoblacion_tbl &lt;- genera_datos(n = 50000, tipo = \"poblacion\")\n# calcular óptimo\npreds_graf_tbl &lt;- poblacion_tbl |&gt; \n  group_by(x) |&gt; # condicionar a x\n  summarise(.pred = mean(y)) |&gt; # media en cada grupo\n  mutate(predictor = \"_óptimo\")\n# graficar con una muestra grande\nggplot(datos_tbl, aes(x = x)) +\n  geom_jitter(aes(y = y), colour = \"red\") + \n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), \n    linewidth = 1.1) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "href": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Estimando el desempeño y datos de prueba",
    "text": "2.2 Estimando el desempeño y datos de prueba\nPara obtener una estimación de la pérdida para una función \\(f\\) que usamos para hacer predicciones, podemos tomar una muestra de datos del proceso generador:\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\nCompararíamos entonces las respuestas observadas \\(\\mathbf{y^{(i)}}\\) con las predicciones \\(f(\\mathbf{x^{(i)}})\\). Ahora resumimos evaluando el error promedio sobre los datos de prueba. El error de prueba de \\(f\\) es\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , f(\\mathbf{x}^{(i)}))\\] Por ejemplo, si usamos la pérdida cuadrática,\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{y}^{(i)} - f(\\mathbf{x}^{(i)}))^2\\] Si \\(m\\) es grande, entonces tenemos por la ley de los grandes números que \\[Err(f) \\approx \\widehat{Err} (f)\\] Podemos también estimar el error de estimación de \\(\\widehat{Err}(f)\\) con técnicas estándar, por ejemplo bootstrap o aproximación normal.\nObervación: nótese que en estos cálculos no es necesario hacer ningún supuesto acerca de \\(f\\), que en este argumento está fija y no utiliza la muestra de prueba.\n\nEjemplo: óptimo\nSupongamos que \\(f\\) es el predictor óptimo que obtuvimos arriba (pero esto aplica para cualquier otra función \\(f\\) que usemos para hacer predicciones). Tomamos una muestra de prueba, y evaluamos usando la raíz de la pérdida cuadrática media:\n\nprueba_tbl &lt;- genera_datos(n = 2000, tipo = \"prueba\")\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen &lt;- function(resumen_tbl){\n  resumen_tbl |&gt; \n    select(-.estimator) |&gt; \n    pivot_wider(names_from = tipo, values_from = .estimate) |&gt; \n    gt() |&gt; \n    fmt_number(where(is_double), decimals = 0)\n}\nfmt_resumen(resumen_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nprueba\n\n\n\n\n_óptimo\nrmse\n49\n\n\n\n\n\n\n\nEste es nuestro error de prueba. Como la muestra de prueba no es muy grande, podríamos usar un método estándar para estimar su precisión, por ejemplo con bootstrap.\n\n\nEjemplo: regla\nAhora probemos con otro predictor, por ejemplo, supongamos que estamos usando la regla de “cada año de escolaridad aumenta ingresos potenciales en 20 unidades”, un predictor construido con reglas manuales que es\n\nf_regla &lt;- function(x){\n  20 * x\n}\n\nAbajo lo graficamos en comparación con el modelo óptimo:\n\naños_x &lt;- tibble(x = seq(0, 17, by = 0.5))\npreds_regla_tbl &lt;- años_x |&gt; \n  mutate(.pred = f_regla(x), predictor = \"regla\")\npreds_graf_tbl &lt;- bind_rows(preds_regla_tbl, preds_graf_tbl)\nggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.2) +\n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\", relationship = \"many-to-many\") \nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nprueba\n\n\n\n\n_óptimo\nrmse\n49\n\n\nregla\nrmse\n91\n\n\n\n\n\n\n\nObserva que el error es considerablemente mayor que el error que obtuvimos con el predictor óptimo del ejemplo anterior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#aprendizaje",
    "href": "02-principios-supervisado.html#aprendizaje",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.3 Aprendizaje supervisado",
    "text": "2.3 Aprendizaje supervisado\nEn aprendizaje supervisado, buscamos construir la función \\(f\\) de manera automática usando datos, de forma que el error de predicción sea lo menor posible. Supongamos entonces que tenemos un conjunto de datos etiquetados (sabemos la \\(y\\) correspondiente a cada \\(x\\)):\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nque llamamos conjunto de entrenamiento.\nUn algoritmo de aprendizaje (aprender de los datos automáticamente) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\):\n\\[{\\mathcal L} \\to \\hat{f} = f_{\\mathcal L} \\]\nUna vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones.\nEl desempeño del predictor particular \\(\\hat{f}\\) se mide igual que antes: observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba,\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\ny calculamos el error de prueba. Si suponemos que \\(m\\) es suficientemente grande:\n\\[ \\widehat{Err}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , \\hat{f}(\\mathbf{x}^{(i)})) \\]\nes una buena aproximación del error de predicción \\(Err(\\hat{f})\\).\nAdicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como\n\\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N L(y^{(i)} , \\hat{f}(x^{(i)})).\\] que es una medida de qué tan bien se ajusta a \\(\\hat{f}\\) a los datos con los que se entrenó \\(\\hat{f}\\). Usualmente esta cantidad no es apropiada para medir el desempeño de un predictor, pues el algoritmo \\(\\hat{f}\\) incluye las “respuestas” \\(y_i\\) en su construcción, de forma que tiende a ser una estimación optimista del error de predicción.\n\nEjemplo: vecinos más cercanos\nConsideremos usar un método de \\(k\\)-vecinos más cercanos para resolver este problema. Este método es simple: si queremos hacer una predicción en las entradas \\(x\\), buscamos los puntos de entrenamiento con entradas \\(x^{(i)}\\) más cercanas a \\(x\\), que denotamos como \\(N_k(x)\\). Tomamos las \\(y\\) correspondientes a estas \\(x\\) y las usamos para hacer nuestra predicción:\n\\[f_2(x) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x)} y^{(i)}\\]\nPrimero obtendremos una muestra de entrenamiento:\n\nset.seed(12)\nentrena_tbl &lt;- genera_datos(n = 20, tipo = \"entrena\")\n\nEn nuestro ejemplo, utilizaremos 2 vecinos más cercanos:\n\n# modelo\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = 2, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\n# preprocesamiento\nreceta &lt;- recipe(y ~ x, data = entrena_tbl |&gt; select(x, y))\n# flujo\nflujo &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\n# Ajustamos flujo\nflujo_ajustado_vecinos &lt;- fit(flujo, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_vmc_tbl &lt;- \n  predict(flujo_ajustado_vecinos, eval_tbl) |&gt; \n  mutate(predictor = \"vecinos\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_vmc_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nentrena\nprueba\n\n\n\n\nvecinos\nrmse\n36\n65\n\n\n\n\n\n\n\nEl error de prueba, que es el que nos interesa hacer chico, es considerablemente grande. Si graficamos podemos ver el problema:\n\npreds_vmc &lt;- predict(flujo_ajustado_vecinos, años_x) |&gt; \n  bind_cols(años_x) |&gt; mutate(predictor = \"vecinos\")\npreds_graf_tbl &lt;- bind_rows(preds_vmc, preds_graf_tbl |&gt; \n  filter(predictor == \"_óptimo\"))\ng_1 &lt;- ggplot(entrena_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor != \"regla\"), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  geom_point(aes(y = y), colour = \"red\") +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\n\n\n\n\nDonde vemos que este método intenta interpolar los datos, capturando ruido y produciendo variaciones que lo alejan del modelo óptimo. Esto lo notamos en lo siguiente:\n\nHay una brecha grande entre el error de entrenamiento y el error predictivo.\nEsta estimación de vecinos más cercanos es muy dependiente de la muestra de entrenamiento que obtengamos, pues intenta casi interpolar los datos. Esto sugiere alta variabilidad de las predicciones dependiendo de la muestra particular de entrenamiento que utilizamos.\nDecimos que este predictor está sobreajustado.\n\n\n\nEjemplo: regresión lineal\nAhora intentaremos con un modelo lineal. En este caso, utilizamos un predictor de la forma\n\\[f(x) = \\beta_0 + \\beta_1x\\] Usamos la muestra de entrenamiento para encontrar la \\(\\beta_0\\) y \\(\\beta_1\\) que minimizar el error sobre los datos disponibles de entrenamiento, lo cual es un problema de optimización relativamente fácil. Usamos entonces \\[\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] para hacer nuestras predicciones.\n\nmodelo_lineal &lt;- linear_reg() |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lm\")\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_lineal_tbl &lt;- \n  predict(flujo_ajustado_lineal, eval_tbl) |&gt; \n  mutate(predictor = \"lineal\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl))\n\n\n\n\n\n\n\npredictor\n.metric\nentrena\nprueba\n\n\n\n\nvecinos\nrmse\n36\n65\n\n\nlineal\nrmse\n49\n56\n\n\n\n\n\n\n\nY el desempeño de este método es mejor que vecinos más cercanos (ver columna de prueba).\n\npreds_1 &lt;- predict(flujo_ajustado_lineal, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\npreds_graf_tbl &lt;- bind_rows(preds_1, preds_graf_tbl)\ng_1 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.1) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor %in% c(\"_óptimo\", \"lineal\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\n\n\n\n\nEn este caso:\n\nNo hay brecha tan grande entre el error de entrenamiento y el error predictivo\nObservamos patrones claros de desajuste: el predictor lineal no captura el patrón curvo que presentan los datos: en la parte media de las \\(x\\) tiende a producir predicciones demasiado altas y lo contario ocurre en los extremos\nDecimos que esté modelo presenta subajuste.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "href": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.4 Entendiendo el error de predicción",
    "text": "2.4 Entendiendo el error de predicción\nPara formalizar las observaciones y definiciones anteriores podemos recurrir a la teoría estadística. Aunque consideraremos aquí por facilidad el error cuadrático, los conceptos presentados nos darán una guía para entender cómo mejorar el desempeño de algoritmos de predicción en general.\nCconsideramos entonces los residuales de cada ajuste, para un caso de prueba:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\] Esta cantidad puede tener un valor positivo o negativo grande, lo que indica errores grandes. Sea \\(f^*\\) el predictor óptimo, que en el caso de la pérdida cuadrática, como explicamos arriba, es \\(f^*(x) = E[Y|X = x]\\) . Entonces, en primer lugar:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{(f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{reducible} + \\underbrace{(\\mathbf{y}- f^*(\\mathbf{x}))}_\\text{irreducible}.\\] donde vemos que si las dos cantidades de la derecha están cercanas a cero, entonces el residual es cercano a cero (la predicción es precisa):\n\nError irreducible: no depende de nuestro algoritmo, sino de la información que tenemos en \\(x\\) para predecir \\(y\\). Si queremos hacer esté error más chico, necesitamos incluir otras variables \\(x\\) relevantes para predecir \\(y\\).\nError reducible: qué tan lejos nuestro método está del óptimo. Podemos mejorar este error seleccionando nuestro algoritmo de predicción \\(\\hat{f}\\) (y el tamaño de la muestra de entrenamiento) de manera adecuada.\n\nEn nuestros dos ejemplos anteriores, el error reducible era considerablemente grande (como podemos verificar comparando con el predictor óptimo, que sólo sufre de error irreducible). Pero la razón por la que ese error reducible es grande es diferente en cada caso.\nSi consideramos pérdida cuadrática, podemos calcular la pérdida promedio condicional a \\(\\mathbf{x}\\) promediando sobre la condicional de \\(\\mathbf{y}\\) dada \\(\\mathbf{x}\\):\n\\[Err(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) = E[(\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))^2] = (f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))^2 + E[(\\mathbf{y}- f^*(\\mathbf{x}))^2]\\] Que reescribimos como:\n\\[\nErr(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))  = (f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))^2 + \\sigma^2(\\mathbf{x})\n\\]\nTeóricamente es dificil decir algo acerca del primer término del lado derecho de esta ecuación. Sin embargo, podemos avanzar si tomamos el promedio sobre todas las muestras posibles de entrenamiento de tamaño \\(n\\) que podríamos usar para ajustar \\(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\). Tomando este valor esperado podemos demostrar que el error promedio se escribe como\n\\[ (f^*(x) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})))^2 + \\textrm{Var}(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + \\sigma^ 2(\\mathbf{x})\\]\nque reescribimos como\n\\[\\textrm{Error(x)} = \\textrm{Sesgo}^2(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + \\textrm{Var}(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + \\sigma^ 2(\\mathbf{x})\\]\nEl error reducible ahora se descompone en dos partes:\n\nEl error por sesgo: que se debe a que nuestro algoritmo sistemáticamente estima mal el valor óptimo.\nEl error por varianza: si las predicciones varían con la muestra, quiere decir que nuestro algoritmo es muy sensible a los datos de entrenamiento, y por lo tanto es difícil que se mantenga cerca del óptimo \\(f^*(x)\\).\n\nEn general, decimos que un algoritmo sufre de sobreajuste si el error por varianza es grande, y sufre de subajuste si el error por sesgo es grande.\nEn nuestros dos ejemplos, intuímos que vecinos más cercanos sufre más de sobreajuste y regresión lineal de subajuste, lo cual verificamos a continuación.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "href": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.5 Ejemplo: fuentes de error",
    "text": "2.5 Ejemplo: fuentes de error\nVamos a ver qué sucede con nuestros dos métodos si utilizamos varias muestras:\n\nset.seed(82)\n\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = 2, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\nflujo_vecinos &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\n\najustes &lt;- map(1:5, function(rep){\n  muestra_tbl &lt;- sample_n(poblacion_tbl, 20) |&gt; \n    mutate(tipo = \"entrena\")\n  flujo_ajustado_vecinos &lt;- fit(flujo_vecinos, muestra_tbl)\n  flujo_ajustado_lineal &lt;- fit(flujo_lineal, muestra_tbl)\n  list(vmc = flujo_ajustado_vecinos, \n       lineal = flujo_ajustado_lineal)\n})\n\nAhora podemos graficar para entender mejor qué está pasando:\n\ndatos_tbl &lt;- map_df(1:length(ajustes), function(i){\n  ajuste &lt;- ajustes[[i]]\n  preds_1 &lt;- predict(ajuste$vmc, tibble(x = 0:17)) |&gt; \n    bind_cols(tibble(x = 0:17, predictor = \"vecinos\"))\n  preds_2 &lt;- predict(ajuste$lineal, tibble(x = 0:17)) |&gt; \n    bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\n  preds_graf_tbl &lt;- bind_rows(preds_1, preds_2) |&gt; \n    mutate(predictor = factor(predictor)) |&gt; \n    mutate(rep = i)\n  preds_graf_tbl\n})\n\n\ng_1 &lt;- ggplot(datos_tbl, aes(x = x, y = .pred)) +\n  geom_line(aes(group = rep, colour = factor(rep))) + facet_wrap(~predictor) +\n  geom_line(data = preds_graf_tbl |&gt; \n            filter(predictor == \"_óptimo\") |&gt; select(-predictor), size = 1.1) +\n  theme(legend.position = \"none\") \ng_1\n\n\n\n\n\n\n\n\nEsto patrón sugiere que, para las muestras que estamos considerando:\n\nNuestro método de vecinos más cercanos tiene capacidad para aprender patrones, pero tiene error considerable por sobreajuste.\nNuestro método lineal no varía mucho, pero tiene errores consistentes sobre todas las muestras. Su error se debe más a falta de capacidad o sesgo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#diagnósticos-básicos-de-sobreajuste-y-subajuste",
    "href": "02-principios-supervisado.html#diagnósticos-básicos-de-sobreajuste-y-subajuste",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.6 Diagnósticos básicos de sobreajuste y subajuste",
    "text": "2.6 Diagnósticos básicos de sobreajuste y subajuste\nPara que exista sobreajuste, la muestra debe poder empujar el algoritmo lejos del óptimo, en el esfuerzo por acercar los valores de la muestra de entrenamiento \\(\\hat{f}_{\\mathcal L}(x^{(i)})\\) a los respectivos \\(y^{(i)}\\). Por otro lado, cuando hay subajuste el algoritmo ajusta mal los valores observados en la muestra de entrenamiento.\nSignos típicos de sobreajuste son:\n\nEl error de entrenamiento es considerablemente menor que el error de prueba. Aunque el error de entrenamiento es aceptable, el error de prueba es demasiado alto.\nLos valores de \\(\\hat{f}_{\\mathcal L}(\\mathbf{x})\\) dependen fuertemente de los valores individuales observados en la muestra de entrenamiento \\(\\mathcal{L}\\) (lo cual idealmente no debería suceder, pues predicciones buenas no deberían cambiar mucho dependiendo de la muestra de entrenamiento).\nNuestro proceso \\(\\mathcal{L}\\to \\hat{f}_{\\mathcal L}\\) generalmente tiene un exceso de capacidad que permite al modelo aprender ruido en el conjunto de entrenamiento.\nEl tamaño de muestra de entrenamiento es relativamente chico (en relación con la complejidad del modelo).\n\nPor otro lado, cuando existe subajuste,\n\nEl error de entrenamiento no es necesariamente lejano del error de prueba, y ambos son altos.\nLos valores de \\(\\hat{f}_{\\mathcal L}(\\mathbf{x})\\) son menos dependiendientes de los valores individuales observados en la muestra de entrenamiento \\(\\mathcal{L}\\) (pues el modelo está cerca de saturar su capacidad, y no muy lejano a lo que obtendríamos con una muestra muy grande).\nEl proceso \\(\\mathcal{L}\\to \\hat{f}_{\\mathcal L}\\) tiene poca capacidad, y no puede aprender patrones claros y estables en los datos.\nEl tamaño de muestra de entrenamiento es relativamente grande (en relación con la complejidad del modelo).\n\nUna algoritmo particular puede sufrir principalmente de sobreajuste o de subajuste, pero también puede sufrir de ambos problemas en distintas regiones del espacio de entradas donde queremos hacer predicciones.\n\n\n\n\n\n\nComplejidad y error de predicción\n\n\n\nPara un tamaño de muestra de entrenamiento fijo,\n\nMétodos de predicción más flexibles o complejos tienden a sufrir más de sobreajuste, pues pueden sobreadaptarse fácilmente a datos observados.\nMétodos de predicción más rígidos o simples tienden a sufrir más error por sesgo, pues tienen menos capacidad para aprender patrones.\n\nIncrementar el tamaño de muestra generalmente mejora las predicciones, principalmente para algoritmos sobreajustados con muestra chica. Cuando el principal problema es subajuste, aumentar el tamaño de muestra puede ayudar menos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "href": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.7 Agregando más información y error irreducible",
    "text": "2.7 Agregando más información y error irreducible\nPodemos ver qué sucede cuando tenemos disponibles más variables relevantes. En este caso, probaremos con dos entradas:\n\ngenera_datos_2 &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x_1 = estudio_años, x_2 = nse,  y = ingreso)\n}\n\n\nentrena_tbl &lt;- genera_datos_2(20, tipo = \"entrena\")\nprueba_tbl &lt;- genera_datos_2(500, tipo = \"prueba\")\nreceta_2 &lt;- recipe(y ~ x_1 + x_2, data = entrena_tbl)\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta_2) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\npredict(flujo_ajustado_lineal, bind_rows(entrena_tbl, prueba_tbl)) |&gt; \n  bind_cols(bind_rows(entrena_tbl, prueba_tbl)) |&gt;\n  group_by(tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(.estimate, decimals = 1)\n\n\n\n\n\n\n\ntipo\n.metric\n.estimator\n.estimate\n\n\n\n\nentrena\nrmse\nstandard\n30.5\n\n\nprueba\nrmse\nstandard\n37.0\n\n\n\n\n\n\n\nY vemos cómo inmediatamente redujimos el error de predicción: en este caso, aunque la variabilidad aumentó un poco (tenemos más parámetros que estimar vs el modelo con una sola variable), la reducción en el sesgo y en el error irreducible es tan grande que el desempeño es muy superior. Examina el caso de vecinos más cercanos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "href": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.8 Acerca de la estimación del error de predicción",
    "text": "2.8 Acerca de la estimación del error de predicción\nCuando usamos una muestra de prueba limitada, podemos evaluar la precisión de nuestra estimación del error de predicción usando por ejemplo el bootstrap. En nuestro ejemplo anterior podríamos hacer los siguiente:\n\nlibrary(infer)\npreds &lt;- predict(flujo_ajustado_lineal, bind_rows(prueba_tbl)) |&gt; \n  bind_cols(prueba_tbl) \npreds |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  group_by(replicate, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; \n  select(replicate, tipo, stat = .estimate) |&gt;\n  get_ci(level = 0.90) |&gt; \n  gt() |&gt; fmt_number(where(is_double), decimals = 1)\n\n\n\n\n\n\n\nlower_ci\nupper_ci\n\n\n\n\n35.1\n38.6",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#resumen",
    "href": "02-principios-supervisado.html#resumen",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.9 Resumen",
    "text": "2.9 Resumen\n\n\n\n\n\n\nTarea fundamental del análisis supervisado\n\n\n\n\nUsando datos de entrenamiento \\({\\mathcal L}\\), construimos una funcion \\(\\hat{f}\\) para predecir. Estas funciones se ajustan usualmente intentando estimar directamente el predictor óptimo \\(f^*(x)\\) (si lo conocemos teóricamente), o indirectamente intentando minimizar la pérdida sobre el conjunto de entrenamiento.\nSi observamos nuevos valores \\(\\mathbf{x}\\), nuestra predicción es \\(\\hat{y} = \\hat{f}(\\mathbf{x})\\).\nBuscamos que cuando observemos nuevos casos para predecir, nuestro error de predicción sea bajo en promedio (\\(Err\\) sea bajo).\nUsualmente estimamos \\(Err\\) mediante una muestra de prueba o validación \\({\\mathcal T}\\).\nNos interesan métodos de construir \\(\\hat{f}\\) que produzcan errores de predicción bajos.\n\n\n\n\nNótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de predicción se estima usando una muestra independiente \\({\\mathcal T}\\).\n\\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción.\nNótese también que aunque generalmente podemos ajustar reduciendo el error de entrenamiento, lo que queremos es reducir el error de prueba: es decir, el error fuera de la muestra de entrenamiento.\n\n\n\n\n\n\n\nReduciendo el error de predicción\n\n\n\nPara reducir el error de predicción, podemos:\n\nIncluir variables relevantes que reduzcan el error irreducible\nReducir variabilidad usando métodos más estables o menos complejos\nReducir sesgo usando métodos más flexibles\nUsar métodos con la estructura adecuada para el problema\n\nGeneralmente 2 y 3 están en contraposición, a lo que muchas veces se le llama equilibrio de varianza y sesgo. Los puntos 1 y 4 generalmente mejoran los resultados reduciendo tanto sesgo como variabilidad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#muestras-de-validación-y-sobreajuste-de-prueba",
    "href": "02-principios-supervisado.html#muestras-de-validación-y-sobreajuste-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.10 Muestras de validación y sobreajuste de prueba",
    "text": "2.10 Muestras de validación y sobreajuste de prueba\nCuando probamos muchos métodos distintos, o varios profesionales intentan obtener buenas calificaciones en un conjunto de prueba, puede ser que aparezca sobreajuste al conjunto de prueba. Una manera de ver esto es que cuando hacemos una gran cantidad de intentos, y buscamos el mínimo error de prueba, es probable que los ganadores no generalizen tan bien a datos nuevos fuera de este conjunto de prueba, o dicho de otra forma, estamos de alguna manera usando los datos de prueba para entrenar. Adicionalmente, los métodos que utilizamos muchas veces tienen hiperparámetros que hay que afinar, lo que implica tener que hacer varias pruebas con datos fuera de los datos de entrenamiento (por ejemplo, un número apropiado de vecinos cercanos). Esto puede invalidar la estimación de error de prueba.\nLa solución para prevenir este sobreajuste es introducir una muestra de prueba adicional que llamamos muestra de validación. Por ejemplo, para un problema donde tenemos datos abundantes, podriamos dividir nuestros datos de la siguiente manera:\n\nLa razón de este proceso es que así podemos ir y venir entre entrenamiento y validación, buscando mejores enfoques y modelos, y no ponemos en riesgo la estimación final del error, o evaluación de calibración de intervalos o probabilidades. Por ejemplo, podemos dividir la muestra en 50-25-25 por ciento. Ajustamos modelos con el primer 50%, evaluamos y seleccionamos con el segundo 25% y finalmente, si es necesario, evaluamos el modelo final seleccionado con la muestra final de 25%.\nEl siguiente resumen explica las propiedades que requerimos en los distintos tipos de meustra:\n\n\n\n\n\n\nPropiedades de entrenamiento-validación-prueba\n\n\n\n\nNo hay una regla general para decidir el tamaño del conjunto de entrenamiento. Generalmente conjuntos de datos más grandes y seleccionados apropiadamente dan mejores resultados.\nLos conjuntos de validación y prueba deben ser preferentemente muestras aleatorias de las poblaciones para las que queremos hacer predicciones, para tener una estimación razonable (no sesgada por ejemplo) del desempeño futuro de nuestro modelo.\nEn cuanto a tamaño, en validación y prueba requerimos que sean de tamaño absoluto suficiente para tener una estimación con precisión suficientemente buena del error y del desempeño predictivo general de nuestros modelos (en el proceso de selección, con el conjunto de validación, y en la evaluación final, con el conjunto de prueba).\nEn validación y prueba, estamos haciendo una pregunta de inferencia: ¿de qué tamaño y bajo que diseño debe ser extraida la muestra de validación y prueba?\n\n\n\nNótese que en validación y prueba es donde la estadística es importante.\nPor ejemplo: supongamos que hacemos un modelo para predecir impago. Quizá nuestro conjunto de entrenamiento tiene pocas personas de cierta región del país. Esto no quiere decir que no podamos aplicar nuestros métodos, siempre y cuando nuestra muesra de validación/prueba tenga suficientes ejemplos para asegurarnos de que nuestro desempeño no es malo en esas regiones (y por tanto produce resultados indeseables en la toma de decisiones).\nPara decidir de antemano los tamaños de validación y prueba, tenemos que tener una idea de qué tanto varía el error de caso a caso (la varianza), si queremos hacer estimaciones en subgrupos de datos (que requerirán tamaño de muestra suficiente también), y cuánto es aceptable como error de estimación del desempeño predictivo, que generalmente no queremos que sea más del 25%, por ejemplo. En caso de usar muestras relativamente chicas de validación o prueba, es necesario estimar el error de estimación del desempeño.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "href": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.11 Resolver problemas con aprendizaje automático",
    "text": "2.11 Resolver problemas con aprendizaje automático\nEn este curso nos concentraremos en la construcción, evaluación y mejora de modelos predictivos. Para que estas ideas funcionen en problemas reales, hay más aspectos a considerar que no discutiremos con tanto detalle, pues en general están muy ligados al problema particular de predicción que nos interesa (y muchas veces son considerablemente más difíciles de la teoría y los algoritmos):\n\nPara entender exactamente cuál es el problema que queremos resolver se requiere trabajo analítico considerable, y también trabajo en entender aspectos del área o negocio donde nos interesa usar aprendizaje máquina. Muchas veces es fácil resolver un problema muy preciso, que tenemos a la mano, pero que más adelante nos damos cuenta de que no es útil.\nEl punto anterior indentificar las métricas que queremos monitorear y mejorar, lo cual no siempre es claro. Hablaremos de este aspecto sólo desde un punto vista técnico. Optimizar métricas incorrectas es poco útil en el mejor de los casos, y en los peores pueden causar daños. Evitar esto requiere monitoreo constante de varios aspectos del funcionamiento de nuestros modelos y sus consecuencias.\n¿Cómo poner en producción modelos y mantenerlos? Un flujo apropiado de trabajo, que comienza con pipelines de preproceso y heurísticas simples, para después utilizar modelos de aprendizaje automático, seguido de monitoreo y entrenamiento continuo son cruciales para tener éxito con este enfoque.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html",
    "href": "03-metodos-locales.html",
    "title": "3  Estructura en modelos y métodos locales",
    "section": "",
    "text": "3.1 Controlando complejidad\nDe la discusión de la sección anterior, y examinando el método de \\(k\\) vecinos más cercanos, puede dar la impresión de que si tenemos suficientes datos, métodos locales como \\(k\\) vecinos pueden ser superiores a otros métodos más estructurados como regresión lineal, que necesariamente incurren en sesgo porque su estructura siempre está mal especificada.\nSin embargo, no es necesario que se cumpla exactamente el supuesto lineal para que los predictores lineales funcionen de manera predictiva, y veremos que en casos típicos los métodos locales simples como \\(k\\)-vecinos más cercanos rara vez funcionan apropiadamente.\nPrimero examinamos cómo controlamos el nivel de complejidad para un método local como \\(k\\) vecinos más cercanos. La idea es que:\nComenzamos con un ejemplo simple en dimensión baja:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructura en modelos y métodos locales</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#controlando-complejidad",
    "href": "03-metodos-locales.html#controlando-complejidad",
    "title": "3  Estructura en modelos y métodos locales",
    "section": "",
    "text": "Más complejidad: Si tomamos \\(k\\) chica, cada estimación usa pocos datos y puede ser ruidosa (incurrimos en variabilidad). Sin embargo, el predictor resultante puede ajustarse a patrones locales y globales.\nMenos complejidad: Si tomamos \\(k\\) grande, cada estimación usa potencialmente datos no relevantes muy lejanos a donde queremos predecir (incurrimos en sesgo), sin embargo cada estimación es más estable pues utiliza más datos.\n\n\n\nEjemplo\n\nlibrary(tidyverse)\nlibrary(gt)\nauto &lt;- read_csv(\"../datos/auto.csv\")\n# seleccionar variables y poner en sistema métrico\ndatos &lt;- auto |&gt; \n  select(name, weight, year, mpg, displacement) |&gt; \n  mutate(\n    peso_kg = weight * 0.45359237,\n    rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n    año = year\n  )\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split &lt;- initial_split(datos, prop = 0.75)\ndatos_entrena &lt;- training(datos_split)\ndatos_prueba &lt;- testing(datos_split)\nnrow(datos_entrena)\n\n[1] 294\n\nnrow(datos_prueba)\n\n[1] 98\n\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, \n  aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\n\n\n\n\nProbaremos con varios valores para \\(k\\), el número de vecinos más cercanos. La función de predicción ajustada es entonces:\n\n# nótese que normalizamos entradas - esto también es importante\n# hacer cuando hacemos vecinos más cercanos, pues en otro caso\n# las variables con escalas más grandes dominan el cálculo\nvmc_1 &lt;- nearest_neighbor(neighbors = tune(), weight_func = \"gaussian\") |&gt;  \n  set_engine(\"kknn\") |&gt;  \n  set_mode(\"regression\")\nreceta_vmc &lt;- recipe(\n  rendimiento_kpl ~ peso_kg + año, datos_entrena) |&gt; \n  step_normalize(all_predictors()) \nflujo_vecinos &lt;- workflow() |&gt;  \n  add_recipe(receta_vmc) |&gt; \n  add_model(vmc_1)\n# definir parámetros que nos interesa explorar\nvecinos_params &lt;- parameters(neighbors(range = c(1, 100)))\n# definir cuáles valores de los parámetros exploramos\nvecinos_grid &lt;- grid_regular(vecinos_params, levels = 100)\nmis_metricas &lt;- metric_set(rmse)\n\nEn la siguiente gráfica mostramos cómo cambia el error de los las predicciones sobre la muestra de validación separada de la de entrenamiento.\n\nr_split &lt;- manual_rset(list(datos_split), \"validación\")\nvecinos_eval_tbl &lt;- tune_grid(flujo_vecinos,\n                            resamples = r_split,\n                            grid = vecinos_grid,\n                            metrics = mis_metricas) \nvecinos_ajustes_tbl &lt;- vecinos_eval_tbl |&gt;\n  unnest(cols = c(.metrics)) |&gt; \n  select(id, neighbors, .metric, .estimate)\nggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) +\n  geom_line() + geom_point() +\n  ylab(\"Error de validación\") + xlab(\"Vecinos\")\n\n\n\n\n\n\n\n\nDonde obtenemos más o menos lo que esperaríamos: modelos con muy pocos vecinos o demasiados vecinos se desempeñan relativamente mal (¿Por qué? Explica para distintos valores de \\(k\\) que tipo de error domina, el sesgo o la varianza).\nSeleccionaremos el mejor modelo según el error estimado de predicción y visualizamos primero nuestras predicciones y los datos de entrenamiento de la siguiente forma:\n\nmejor_rmse &lt;- select_best(vecinos_eval_tbl, metric = \"rmse\")\najuste_1 &lt;- finalize_workflow(flujo_vecinos, mejor_rmse) |&gt; \n  fit(datos_entrena)\ndat_graf &lt;- tibble(peso_kg = seq(900, 2200, by = 10)) |&gt; \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf &lt;- dat_graf |&gt; \n  mutate(pred_1 = predict(ajuste_1, dat_graf) |&gt; pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  linewidth = 1.2)\n\n\n\n\n\n\n\n\nEl método parece funcionar razonablemente bien para este problema simple. Sin embargo, si el espacio de entradas no es de dimensión baja, entonces podemos encontrarnos con dificultades.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructura en modelos y métodos locales</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "href": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "title": "3  Estructura en modelos y métodos locales",
    "section": "3.2 La maldición de la dimensionalidad",
    "text": "3.2 La maldición de la dimensionalidad\nEl método de k-vecinos más cercanos funciona mejor cuando\n\nNo es necesario hacer \\(k\\) demasiado grande, de forma que terminemos tomando valores lejanos que inducen sesgo.\nNo es necesario hacer \\(k\\) demasiado chica, de forma que nuestras predicciones sean inestables.\n\n\n\n\n\n\n\nMaldición de la dimensionalidad\n\n\n\nEn dimensión alta, para la mayoría de las \\(\\mathbf{x}\\) donde queremos hacer predicciones típicamente no existen vecinos cercanos, aún para conjuntos de entrenamiento muy grandes.\n\n\nEsto implica que para tamaños típicos \\(n\\) de muestra de entrenamiento:\n\nSi tomamos \\(k\\) más grande, el sesgo es grande.\nSi tomamos \\(k\\) chica, el sesgo puede ser grande pues estamos de todas formas obligados a buscar vecinos lejos de donde queremos predecir. La variabilidad también es alta pues usamos pocos datos para cada predicción.\nPara que una \\(k\\) chica tenga sesgo de estimación bajo, el tamaño \\(n\\) de la muestra de entrenamiento tiene que ser gigantesca.\n\n\nEjemplo\nConsideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, con una muestra de entrenamiento de 1000 casos. Generamos $x^{i}’s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso:\n\nfun_exp &lt;- function(x) exp(-8 * sum(x ^ 2))\nx &lt;- map(1:1000, ~ runif(2, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n        mutate(y = map_dbl(x, fun_exp))\nggplot(dat |&gt; mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), \n       aes(x = x_1, y = x_2, colour = y)) + geom_point()\n\n\n\n\n\n\n\n\nLa mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). El vecino más cercano al origen es\n\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [2]&gt; 0.995      0.0261\n\nmas_cercano$x[[1]]\n\n[1] -0.025090354  0.007277334\n\n\nNuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.994555, que es bastante cercano al valor verdadero (1).\nAhora intentamos hacer lo mismo para dimensión \\(p=8\\).\n\nx &lt;- map(1:1000, ~ runif(8, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n       mutate(y = map_dbl(x, fun_exp))\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [8]&gt; 0.104       0.532\n\nmas_cercano$x[[1]]\n\n[1]  0.30027994  0.36774993 -0.06613864 -0.03673154  0.12260975  0.16718980\n[7] -0.01866598 -0.09308947\n\n\nY el resultado es un desastre. Nuestra predicción es\n\nmas_cercano$y\n\n[1] 0.1038249\n\n\nNecesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (haz pruebas).\n¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructura en modelos y métodos locales</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "href": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "title": "3  Estructura en modelos y métodos locales",
    "section": "3.3 Regresión lineal en dimensión alta",
    "text": "3.3 Regresión lineal en dimensión alta\nAhora intentamos algo similar con una función que es razonable aproximar con una función lineal:\n\nfun_cuad &lt;- function(x)  0.5 * (1 + x[1])^2\n\nY queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es\n\nfun_cuad(0)\n\n[1] 0.5\n\n\nLos datos se generan de la siguiente forma:\n\nsimular_datos &lt;- function(p = 40){\n    x &lt;- map(1:1000,  ~ runif(p, -1, 1))\n    dat &lt;- tibble(x = x) |&gt; mutate(y = map_dbl(x, fun_cuad)) \n    dat\n}\n\nPor ejemplo para dimensión baja \\(p=1\\) (nótese que una aproximación lineal es razonable):\n\nejemplo &lt;- simular_datos(p = 1) |&gt; mutate(x = unlist(x))\nggplot(ejemplo, aes(x = x, y = y)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAhora repetimos el proceso en dimensión \\(p=40\\): simulamos las entradas, y aplicamos un vecino más cercano\n\nvmc_1 &lt;- function(dat){\n    dat &lt;- dat |&gt; \n        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n        arrange(dist_origen)\n        mas_cercano &lt;- dat[1, ]\n        mas_cercano$y\n}\nset.seed(834)\ndat &lt;- simular_datos(p = 40)\nvmc_1(dat)\n\n[1] 1.206478\n\n\nEste no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor:\n\nregresion_pred &lt;- function(dat){\n    p &lt;- length(dat$x[[1]])\n    dat_reg &lt;- cbind(\n        y = dat$y, \n        x = matrix(unlist(dat$x), ncol = p, byrow=T)) |&gt; \n        as.data.frame()\n    mod_lineal &lt;- lm(y ~ ., dat = dat_reg)\n    origen &lt;- data.frame(matrix(rep(0, p), 1, p))\n    names(origen) &lt;- names(dat_reg)[2:(p+1)]\n    predict(mod_lineal, newdata = origen)\n}\nregresion_pred(dat)\n\n        1 \n0.6677861 \n\n\nLa razón de este mejor desempeño de regresión es que en este caso, el modelo lineal explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente.\nSolución: vamos a hacer varias simulaciones, para ver qué modelo se desempeña mejor.\n\nsims &lt;- map(1:200, function(i){\n    dat &lt;- simular_datos(p = 40)\n    vmc_y &lt;- vmc_1(dat)\n    reg_y &lt;- regresion_pred(dat)\n    tibble(rep = i, \n           error = c(abs(vmc_y - 0.5), abs(reg_y - 0.5)), \n            tipo = c(\"vmc\", \"regresion\"))\n}) |&gt; bind_rows()\nggplot(sims, aes(x = tipo, y = error)) + geom_boxplot() \n\n\n\n\n\n\n\n\nAsí que típicamente el error de vecinos más cercanos es más alto que el de regresión. El error esperado es para vmc es más de doble que el de regresión:\n\nsims |&gt; group_by(tipo) |&gt; \n  summarise(media_error = mean(error)) |&gt; \n  gt()\n\n\n\n\n\n\n\ntipo\nmedia_error\n\n\n\n\nregresion\n0.1662124\n\n\nvmc\n0.3542532\n\n\n\n\n\n\n\nLo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la información para construir predicción con sesgo y varianza bajas. En contraste, vecinos más cercanos sufre de varianza alta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructura en modelos y métodos locales</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#estructura-en-métodos-de-predicción",
    "href": "03-metodos-locales.html#estructura-en-métodos-de-predicción",
    "title": "3  Estructura en modelos y métodos locales",
    "section": "3.4 Estructura en métodos de predicción",
    "text": "3.4 Estructura en métodos de predicción\n\n\n\n\n\n\nMétodos locales sin estructura\n\n\n\nLos métodos locales muchas veces no funcionan bien en dimensión alta. La razón es que:\n\nEl sesgo es alto, pues promediamos puntos muy lejanos al lugar donde queremos predecir (aunque tomemos pocos vecinos cercanos).\nEn el caso de que encontremos unos pocos puntos cercanos, la varianza también puede ser alta porque promediamos relativamente pocos vecinos.\n\nMétodos con más estructura global, apropiada para el problema, logran explotar información de puntos que no están tan cerca del lugar donde queremos predecir.\n\n\nMuchas veces el éxito en la predicción depende de establecer esas estructuras apropiadas ya sea mediante estructura apropiada:\n\nEfectos lineales cuando variables tienen efectos aproximadamente lineales.\nPredictores basados en árboles cuando las interacciones entre variables son importantes.\nRedes neuronales convolucionales para procesamiento de imágenes y señales.\nRedes neuronales para lenguaje que toman el contexto corto y más largo de las palabras que aparecen en un texto.\n\nEsto incluye también un procesamiento adecuado de los datos crudos que utilizamos para hacer predicciones, por ejemplo:\n\nTransformaciones de variables apropiadas (por ejemplo, logaritmos, splines, procesamiento de variables categóricas, etc.).\nSelección de variables relevantes para el problema.\nReducción de dimensionalidad (por ejemplo, embeddings basados en otros modelos, o técnicas como componentes principales/descomposición en valores singulares, etc.).\nProcesamiento apropiado de estructuras complejas de datos (por ejemplo, cuando cada unidad tiene varias mediciones en el tiempo).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructura en modelos y métodos locales</span>"
    ]
  }
]